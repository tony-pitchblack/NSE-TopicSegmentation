{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as PACK, pad_packed_sequence as PAD\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from abc import ABC, abstractmethod\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf ./text-segmentation\n",
    "# !git clone https://github.com/koomri/text-segmentation.git\n",
    "!rm -rf ./data\n",
    "!mkdir -p ./data/choi\n",
    "!cp -r ./text-segmentation/data/choi ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brazen\\Miniconda3\\envs\\default\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# SMALL SBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "nse_model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    }
   ],
   "source": [
    "def batch_calc_docs_embs(batch_docs):\n",
    "        list_docs_embs = []\n",
    "        for doc in batch_docs:\n",
    "            tokenized_docs = tokenizer(\n",
    "                doc,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors='pt',\n",
    "                return_token_type_ids=False,\n",
    "                return_attention_mask=False\n",
    "                )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tokenized_docs = {k: v.to(nse_model.device) for k, v in tokenized_docs.items()}\n",
    "                model_output = nse_model(**tokenized_docs)\n",
    "\n",
    "            docs_embs = 0\n",
    "            docs_embs = model_output.last_hidden_state[:, 0, :]\n",
    "            docs_embs = torch.nn.functional.normalize(docs_embs)\n",
    "            list_docs_embs.append(docs_embs)\n",
    "        batch_docs_embs = pad_sequence(list_docs_embs, batch_first=True)\n",
    "        return batch_docs_embs\n",
    "\n",
    "sample_text = \"\"\"We use the Pk metric as defined in Beeferman\n",
    "et al. (1999) to evaluate the performance of our\n",
    "model. Pk is the probability that when passing a\n",
    "sliding window of size k over sentences, the sentences at the boundaries of the window will be incorrectly classified as belonging to the same segment (or vice versa). To match the setup of Chen\n",
    "et al. (2009), we also provide the Pk metric for a\n",
    "sliding window over words when evaluating on the\n",
    "datasets from their paper\"\"\"\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/russian.pickle')\n",
    "sample_sents = sent_detector.tokenize(sample_text)\n",
    "sample_sents = [sample_sents]\n",
    "sample_lengths = [len(s) for s in sample_sents]\n",
    "sample_lengths = torch.LongTensor(sample_lengths)\n",
    "sample_embs = batch_calc_docs_embs(sample_sents)\n",
    "sample_targets = torch.zeros(1, len(sample_sents[0]))\n",
    "sample_targets[:, sample_targets.shape[1]//2] = 1 # split into two segments in the middle\n",
    "\n",
    "# create dummy batch of copies\n",
    "batch_size = 2\n",
    "sample_embs = sample_embs.expand(batch_size, -1, -1)\n",
    "sample_targets = sample_targets.expand(batch_size, -1)\n",
    "sample_lengths = sample_lengths.expand(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "540.63s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "545.76s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-09 12:55:10.122977: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-06-09 12:55:10.123039: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[nltk_data] Downloading package punkt to /home/brazen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/brazen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/brazen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/brazen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtony-pitchblack\u001b[0m (\u001b[33moverfit1010\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/brazen/.netrc\n",
      "Using architecture with conditional random field layer\n",
      "Train loader has: 593 documents\n",
      "Validation loader has: 65 documents\n",
      "Test loader has: 262 documents\n",
      "Train loader has: 593 documents\n",
      "Validation loader has: 65 documents\n",
      "Test loader has: 262 documents\n",
      "Train loader has: 593 documents\n",
      "Validation loader has: 65 documents\n",
      "Test loader has: 262 documents\n",
      "Train loader has: 593 documents\n",
      "Validation loader has: 65 documents\n",
      "Test loader has: 262 documents\n",
      "Train loader has: 593 documents\n",
      "Validation loader has: 65 documents\n",
      "Test loader has: 262 documents\n",
      "Train loader has: 593 documents\n",
      "Validation loader has: 65 documents\n",
      "Test loader has: 262 documents\n",
      "Train loader has: 593 documents\n",
      "Validation loader has: 65 documents\n",
      "Test loader has: 265 documents\n",
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "filepath = './train_fit.py'\n",
    "# ! chmod 755 {filepath}\n",
    "exp_name = 'choi_test'\n",
    "!rm -rf {exp_name}\n",
    "# !cd {nse_topseg_path} && python train_fit.py --dataset choi -exp {exp_name}\n",
    "! python {filepath} \\\n",
    "    --dataset choi -exp {exp_name} \\\n",
    "    --wandb --wandb_key aee284a72205e2d6787bd3ce266c5b9aefefa42c \\\n",
    "    --online_encoding --metric='F1' --verbose \\\n",
    "    --encoder=\"cointegrated/rubert-tiny2\" \\\n",
    "    --hidden_units=256 --num_layers=2 -lr=0.001 -bs=8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
