{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pack_padded_sequence \u001b[38;5;28;01mas\u001b[39;00m PACK, pad_packed_sequence \u001b[38;5;28;01mas\u001b[39;00m PAD\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as PACK, pad_packed_sequence as PAD\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from abc import ABC, abstractmethod\n",
    "import shutil\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'text-segmentation'...\n",
      "Updating files:  54% (528/973)\n",
      "Updating files:  55% (536/973)\n",
      "Updating files:  56% (545/973)\n",
      "Updating files:  57% (555/973)\n",
      "Updating files:  58% (565/973)\n",
      "Updating files:  59% (575/973)\n",
      "Updating files:  60% (584/973)\n",
      "Updating files:  61% (594/973)\n",
      "Updating files:  62% (604/973)\n",
      "Updating files:  63% (613/973)\n",
      "Updating files:  64% (623/973)\n",
      "Updating files:  65% (633/973)\n",
      "Updating files:  66% (643/973)\n",
      "Updating files:  67% (652/973)\n",
      "Updating files:  68% (662/973)\n",
      "Updating files:  69% (672/973)\n",
      "Updating files:  70% (682/973)\n",
      "Updating files:  71% (691/973)\n",
      "Updating files:  72% (701/973)\n",
      "Updating files:  73% (711/973)\n",
      "Updating files:  74% (721/973)\n",
      "Updating files:  75% (730/973)\n",
      "Updating files:  76% (740/973)\n",
      "Updating files:  77% (750/973)\n",
      "Updating files:  78% (759/973)\n",
      "Updating files:  79% (769/973)\n",
      "Updating files:  80% (779/973)\n",
      "Updating files:  81% (789/973)\n",
      "Updating files:  82% (798/973)\n",
      "Updating files:  83% (808/973)\n",
      "Updating files:  84% (818/973)\n",
      "Updating files:  85% (828/973)\n",
      "Updating files:  86% (837/973)\n",
      "Updating files:  87% (847/973)\n",
      "Updating files:  88% (857/973)\n",
      "Updating files:  89% (866/973)\n",
      "Updating files:  90% (876/973)\n",
      "Updating files:  91% (886/973)\n",
      "Updating files:  92% (896/973)\n",
      "Updating files:  93% (905/973)\n",
      "Updating files:  94% (915/973)\n",
      "Updating files:  95% (925/973)\n",
      "Updating files:  96% (935/973)\n",
      "Updating files:  97% (944/973)\n",
      "Updating files:  98% (954/973)\n",
      "Updating files:  99% (964/973)\n",
      "Updating files: 100% (973/973)\n",
      "Updating files: 100% (973/973), done.\n",
      "\"rm\" �� ���� ����७��� ��� ���譥�\n",
      "��������, �ᯮ��塞�� �ணࠬ��� ��� ������ 䠩���.\n",
      "�訡�� � ᨭ⠪�� �������.\n",
      "\"cp\" �� ���� ����७��� ��� ���譥�\n",
      "��������, �ᯮ��塞�� �ணࠬ��� ��� ������ 䠩���.\n"
     ]
    }
   ],
   "source": [
    "# !rm -rf ./text-segmentation\n",
    "!git clone https://github.com/koomri/text-segmentation.git\n",
    "!rm -rf ./data\n",
    "!mkdir -p ./data/choi\n",
    "!cp -r ./text-segmentation/data/choi ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brazen\\Miniconda3\\envs\\default\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# SMALL SBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "nse_model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    }
   ],
   "source": [
    "def batch_calc_docs_embs(batch_docs):\n",
    "        list_docs_embs = []\n",
    "        for doc in batch_docs:\n",
    "            tokenized_docs = tokenizer(\n",
    "                doc,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors='pt',\n",
    "                return_token_type_ids=False,\n",
    "                return_attention_mask=False\n",
    "                )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tokenized_docs = {k: v.to(nse_model.device) for k, v in tokenized_docs.items()}\n",
    "                model_output = nse_model(**tokenized_docs)\n",
    "\n",
    "            docs_embs = 0\n",
    "            docs_embs = model_output.last_hidden_state[:, 0, :]\n",
    "            docs_embs = torch.nn.functional.normalize(docs_embs)\n",
    "            list_docs_embs.append(docs_embs)\n",
    "        batch_docs_embs = pad_sequence(list_docs_embs, batch_first=True)\n",
    "        return batch_docs_embs\n",
    "\n",
    "sample_text = \"\"\"We use the Pk metric as defined in Beeferman\n",
    "et al. (1999) to evaluate the performance of our\n",
    "model. Pk is the probability that when passing a\n",
    "sliding window of size k over sentences, the sentences at the boundaries of the window will be incorrectly classified as belonging to the same segment (or vice versa). To match the setup of Chen\n",
    "et al. (2009), we also provide the Pk metric for a\n",
    "sliding window over words when evaluating on the\n",
    "datasets from their paper\"\"\"\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/russian.pickle')\n",
    "sample_sents = sent_detector.tokenize(sample_text)\n",
    "sample_sents = [sample_sents]\n",
    "sample_lengths = [len(s) for s in sample_sents]\n",
    "sample_lengths = torch.LongTensor(sample_lengths)\n",
    "sample_embs = batch_calc_docs_embs(sample_sents)\n",
    "sample_targets = torch.zeros(1, len(sample_sents[0]))\n",
    "sample_targets[:, sample_targets.shape[1]//2] = 1 # split into two segments in the middle\n",
    "\n",
    "# create dummy batch of copies\n",
    "batch_size = 2\n",
    "sample_embs = sample_embs.expand(batch_size, -1, -1)\n",
    "sample_targets = sample_targets.expand(batch_size, -1)\n",
    "sample_lengths = sample_lengths.expand(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging into wanbd with key: aee284a72205e2d6787bd3ce266c5b9aefefa42c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 13:50:44.500687: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2024-06-10 13:50:44.507847: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "c:\\Users\\User\\VSCode projects\\NSE-TopicSegmentation\\.venv\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "wandb: Currently logged in as: tony-pitchblack (overfit1010). Use `wandb login --relogin` to force relogin\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\User\\.netrc\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\User\\VSCode projects\\NSE-TopicSegmentation\\train_fit.py\", line 1053, in <module>\n",
      "    main(args)\n",
      "  File \"c:\\Users\\User\\VSCode projects\\NSE-TopicSegmentation\\train_fit.py\", line 81, in main\n",
      "    folds = load_dataset(args.dataset,\n",
      "  File \"c:\\Users\\User\\VSCode projects\\NSE-TopicSegmentation\\utils\\load_datasets.py\", line 69, in load_dataset\n",
      "    dataset = CorusDataset(load_lenta, url,\n",
      "  File \"c:\\Users\\User\\VSCode projects\\NSE-TopicSegmentation\\utils\\corus_dataset.py\", line 18, in __init__\n",
      "    out = subprocess.run(\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\subprocess.py\", line 501, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\subprocess.py\", line 947, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\subprocess.py\", line 1416, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "FileNotFoundError: [WinError 2] Не удается найти указанный файл\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "filepath = './train_fit.py'\n",
    "# ! chmod 755 {filepath}\n",
    "dataset_name = 'lenta'\n",
    "exp_name = dataset_name\n",
    "shutil.rmtree(exp_name, ignore_errors=True)\n",
    "# !rm -rf {exp_name}\n",
    "! python {filepath} \\\n",
    "    --dataset {dataset_name} -exp {exp_name} \\\n",
    "    --wandb --wandb_key=aee284a72205e2d6787bd3ce266c5b9aefefa42c \\\n",
    "    --online_encoding --metric=F1 --verbose \\\n",
    "    --encoder=\"cointegrated/rubert-tiny2\" \\\n",
    "    --hidden_units=256 --num_layers=2 -lr=0.001 -bs=8 \\\n",
    "    --max_docs_cnt=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy.linalg._umath_linalg' has no attribute '_ilp64'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorus_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CorusDataset\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcorus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_lenta\n\u001b[0;32m      4\u001b[0m segments_per_doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\VSCode projects\\NSE-TopicSegmentation\\utils\\corus_dataset.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msubprocess\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      5\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorusDataset\u001b[39;00m(IterableDataset):\n",
      "File \u001b[1;32mc:\\Users\\User\\VSCode projects\\NSE-TopicSegmentation\\.venv\\lib\\site-packages\\nltk\\__init__.py:128\u001b[0m\n\u001b[0;32m    120\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mPopen \u001b[38;5;241m=\u001b[39m _fake_Popen\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# TOP-LEVEL MODULES\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Import top-level functionality into top-level namespace\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollocations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator, memoize\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatstruct\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\VSCode projects\\NSE-TopicSegmentation\\.venv\\lib\\site-packages\\nltk\\collocations.py:39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ngrams\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# these two unused imports are referenced in collocations.doctest\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     40\u001b[0m     ContingencyMeasures,\n\u001b[0;32m     41\u001b[0m     BigramAssocMeasures,\n\u001b[0;32m     42\u001b[0m     TrigramAssocMeasures,\n\u001b[0;32m     43\u001b[0m     QuadgramAssocMeasures,\n\u001b[0;32m     44\u001b[0m )\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspearman\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ranks_from_scores, spearman_correlation\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAbstractCollocationFinder\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\User\\VSCode projects\\NSE-TopicSegmentation\\.venv\\lib\\site-packages\\nltk\\metrics\\__init__.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Natural Language Toolkit: Metrics\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2001-2021 NLTK Project\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# For license information, see LICENSE.TXT\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mNLTK Metrics\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03mClasses and methods for scoring processing modules.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     accuracy,\n\u001b[0;32m     18\u001b[0m     precision,\n\u001b[0;32m     19\u001b[0m     recall,\n\u001b[0;32m     20\u001b[0m     f_measure,\n\u001b[0;32m     21\u001b[0m     log_likelihood,\n\u001b[0;32m     22\u001b[0m     approxrand,\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfusionmatrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfusionMatrix\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     26\u001b[0m     edit_distance,\n\u001b[0;32m     27\u001b[0m     edit_distance_align,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     fractional_presence,\n\u001b[0;32m     35\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\User\\VSCode projects\\NSE-TopicSegmentation\\.venv\\lib\\site-packages\\nltk\\metrics\\scores.py:15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reduce\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m betai\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     betai \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\VSCode projects\\NSE-TopicSegmentation\\.venv\\lib\\site-packages\\scipy\\stats\\__init__.py:391\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    389\u001b[0m \n\u001b[0;32m    390\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 391\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmorestats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\VSCode projects\\NSE-TopicSegmentation\\.venv\\lib\\site-packages\\scipy\\stats\\stats.py:180\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspecial\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distributions\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mstats_basic\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_mstats_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (_find_repeats, linregress, theilslopes,\n\u001b[0;32m    183\u001b[0m                                    siegelslopes)\n",
      "File \u001b[1;32mc:\\Users\\User\\VSCode projects\\NSE-TopicSegmentation\\.venv\\lib\\site-packages\\scipy\\stats\\distributions.py:8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Author:  Travis Oliphant  2002-2011 with contributions from\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#          SciPy Developers 2004-2011\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#       instead of `git blame -Lxxx,+x`.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_distn_infrastructure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (entropy, rv_discrete, rv_continuous,\n\u001b[0;32m      9\u001b[0m                                     rv_frozen)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _continuous_distns\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _discrete_distns\n",
      "File \u001b[1;32mc:\\Users\\User\\VSCode projects\\NSE-TopicSegmentation\\.venv\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (comb, chndtr, entr, rel_entr, xlogy, ive)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# for root finding for continuous distribution ppf, and max likelihood estimation\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimize\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# for functions of continuous distributions (e.g. moments, entropy, cdf)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m integrate\n",
      "File \u001b[1;32mc:\\Users\\User\\VSCode projects\\NSE-TopicSegmentation\\.venv\\lib\\site-packages\\scipy\\optimize\\__init__.py:401\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m=====================================================\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mOptimization and root finding (:mod:`scipy.optimize`)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m:mod:`Additional information on the nonlinear solvers <scipy.optimize.nonlin>`\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m--> 401\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_minimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_root\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_root_scalar\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\VSCode projects\\NSE-TopicSegmentation\\.venv\\lib\\site-packages\\scipy\\optimize\\_minimize.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_trustregion_krylov\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _minimize_trust_krylov\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_trustregion_exact\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _minimize_trustregion_exact\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_trustregion_constr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _minimize_trustregion_constr\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# constrained minimization\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlbfgsb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _minimize_lbfgsb\n",
      "File \u001b[1;32mc:\\Users\\User\\VSCode projects\\NSE-TopicSegmentation\\.venv\\lib\\site-packages\\scipy\\optimize\\_trustregion_constr\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"This module contains the equality constrained SQP solver.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mminimize_trustregion_constr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _minimize_trustregion_constr\n\u001b[0;32m      6\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_minimize_trustregion_constr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\User\\VSCode projects\\NSE-TopicSegmentation\\.venv\\lib\\site-packages\\scipy\\optimize\\_trustregion_constr\\minimize_trustregion_constr.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearOperator\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_differentiable_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorFunction\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_constraints\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     NonlinearConstraint, LinearConstraint, PreparedConstraint, strict_bounds)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_hessian_update_strategy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BFGS\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OptimizeResult\n",
      "File \u001b[1;32mc:\\Users\\User\\VSCode projects\\NSE-TopicSegmentation\\.venv\\lib\\site-packages\\scipy\\optimize\\_constraints.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OptimizeWarning\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m suppress_warnings\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_arr_to_scalar\u001b[39m(x):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# If x is a numpy array, return x.item().  This will\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# fail if the array has more than one element.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\VSCode projects\\NSE-TopicSegmentation\\.venv\\lib\\site-packages\\numpy\\testing\\__init__.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munittest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TestCase\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _private\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_private\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_private\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (_assert_valid_refcount, _gen_alignment_data)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_private\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m extbuild\n",
      "File \u001b[1;32mc:\\Users\\User\\VSCode projects\\NSE-TopicSegmentation\\.venv\\lib\\site-packages\\numpy\\testing\\_private\\utils.py:57\u001b[0m\n\u001b[0;32m     55\u001b[0m IS_PYSTON \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(sys, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyston_version_info\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m HAS_REFCOUNT \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(sys, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgetrefcount\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m IS_PYSTON\n\u001b[1;32m---> 57\u001b[0m HAS_LAPACK64 \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ilp64\u001b[49m\n\u001b[0;32m     59\u001b[0m _OLD_PROMOTION \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: np\u001b[38;5;241m.\u001b[39m_get_promotion_state() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlegacy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     61\u001b[0m IS_MUSL \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy.linalg._umath_linalg' has no attribute '_ilp64'"
     ]
    }
   ],
   "source": [
    "from utils.corus_dataset import CorusDataset\n",
    "from corus import load_lenta\n",
    "\n",
    "segments_per_doc = 20\n",
    "url = 'https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz'\n",
    "dataset = CorusDataset(load_lenta, url,\n",
    "                        segments_per_doc=segments_per_doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
