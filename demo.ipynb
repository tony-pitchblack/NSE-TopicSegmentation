{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pack_padded_sequence \u001b[38;5;28;01mas\u001b[39;00m PACK, pad_packed_sequence \u001b[38;5;28;01mas\u001b[39;00m PAD\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as PACK, pad_packed_sequence as PAD\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from abc import ABC, abstractmethod\n",
    "import shutil\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'text-segmentation'...\n",
      "Updating files:  54% (528/973)\n",
      "Updating files:  55% (536/973)\n",
      "Updating files:  56% (545/973)\n",
      "Updating files:  57% (555/973)\n",
      "Updating files:  58% (565/973)\n",
      "Updating files:  59% (575/973)\n",
      "Updating files:  60% (584/973)\n",
      "Updating files:  61% (594/973)\n",
      "Updating files:  62% (604/973)\n",
      "Updating files:  63% (613/973)\n",
      "Updating files:  64% (623/973)\n",
      "Updating files:  65% (633/973)\n",
      "Updating files:  66% (643/973)\n",
      "Updating files:  67% (652/973)\n",
      "Updating files:  68% (662/973)\n",
      "Updating files:  69% (672/973)\n",
      "Updating files:  70% (682/973)\n",
      "Updating files:  71% (691/973)\n",
      "Updating files:  72% (701/973)\n",
      "Updating files:  73% (711/973)\n",
      "Updating files:  74% (721/973)\n",
      "Updating files:  75% (730/973)\n",
      "Updating files:  76% (740/973)\n",
      "Updating files:  77% (750/973)\n",
      "Updating files:  78% (759/973)\n",
      "Updating files:  79% (769/973)\n",
      "Updating files:  80% (779/973)\n",
      "Updating files:  81% (789/973)\n",
      "Updating files:  82% (798/973)\n",
      "Updating files:  83% (808/973)\n",
      "Updating files:  84% (818/973)\n",
      "Updating files:  85% (828/973)\n",
      "Updating files:  86% (837/973)\n",
      "Updating files:  87% (847/973)\n",
      "Updating files:  88% (857/973)\n",
      "Updating files:  89% (866/973)\n",
      "Updating files:  90% (876/973)\n",
      "Updating files:  91% (886/973)\n",
      "Updating files:  92% (896/973)\n",
      "Updating files:  93% (905/973)\n",
      "Updating files:  94% (915/973)\n",
      "Updating files:  95% (925/973)\n",
      "Updating files:  96% (935/973)\n",
      "Updating files:  97% (944/973)\n",
      "Updating files:  98% (954/973)\n",
      "Updating files:  99% (964/973)\n",
      "Updating files: 100% (973/973)\n",
      "Updating files: 100% (973/973), done.\n",
      "\"rm\" ï¿½ï¿½ ï¢«ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½à¥­ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½è­¥ï¿½\n",
      "ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½, ï¿½á¯®ï¿½ï¿½ï¥¬ï¿½ï¿½ ï¿½à®£à ¬ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ä ©ï¿½ï¿½ï¿½.\n",
      "ï¿½è¨¡ï¿½ï¿½ ï¿½ á¨­â ªï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½.\n",
      "\"cp\" ï¿½ï¿½ ï¢«ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½à¥­ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½è­¥ï¿½\n",
      "ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½, ï¿½á¯®ï¿½ï¿½ï¥¬ï¿½ï¿½ ï¿½à®£à ¬ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ä ©ï¿½ï¿½ï¿½.\n"
     ]
    }
   ],
   "source": [
    "# !rm -rf ./text-segmentation\n",
    "!git clone https://github.com/koomri/text-segmentation.git\n",
    "!rm -rf ./data\n",
    "!mkdir -p ./data/choi\n",
    "!cp -r ./text-segmentation/data/choi ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brazen\\Miniconda3\\envs\\default\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# SMALL SBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "nse_model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    }
   ],
   "source": [
    "def batch_calc_docs_embs(batch_docs):\n",
    "        list_docs_embs = []\n",
    "        for doc in batch_docs:\n",
    "            tokenized_docs = tokenizer(\n",
    "                doc,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors='pt',\n",
    "                return_token_type_ids=False,\n",
    "                return_attention_mask=False\n",
    "                )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tokenized_docs = {k: v.to(nse_model.device) for k, v in tokenized_docs.items()}\n",
    "                model_output = nse_model(**tokenized_docs)\n",
    "\n",
    "            docs_embs = 0\n",
    "            docs_embs = model_output.last_hidden_state[:, 0, :]\n",
    "            docs_embs = torch.nn.functional.normalize(docs_embs)\n",
    "            list_docs_embs.append(docs_embs)\n",
    "        batch_docs_embs = pad_sequence(list_docs_embs, batch_first=True)\n",
    "        return batch_docs_embs\n",
    "\n",
    "sample_text = \"\"\"We use the Pk metric as defined in Beeferman\n",
    "et al. (1999) to evaluate the performance of our\n",
    "model. Pk is the probability that when passing a\n",
    "sliding window of size k over sentences, the sentences at the boundaries of the window will be incorrectly classified as belonging to the same segment (or vice versa). To match the setup of Chen\n",
    "et al. (2009), we also provide the Pk metric for a\n",
    "sliding window over words when evaluating on the\n",
    "datasets from their paper\"\"\"\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/russian.pickle')\n",
    "sample_sents = sent_detector.tokenize(sample_text)\n",
    "sample_sents = [sample_sents]\n",
    "sample_lengths = [len(s) for s in sample_sents]\n",
    "sample_lengths = torch.LongTensor(sample_lengths)\n",
    "sample_embs = batch_calc_docs_embs(sample_sents)\n",
    "sample_targets = torch.zeros(1, len(sample_sents[0]))\n",
    "sample_targets[:, sample_targets.shape[1]//2] = 1 # split into two segments in the middle\n",
    "\n",
    "# create dummy batch of copies\n",
    "batch_size = 2\n",
    "sample_embs = sample_embs.expand(batch_size, -1, -1)\n",
    "sample_targets = sample_targets.expand(batch_size, -1)\n",
    "sample_lengths = sample_lengths.expand(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def delete_experiment(exp_name):\n",
    "    if os.path.exists(exp_name):\n",
    "        ans = input(f\"Experiment '{exp_name}' already exists. Delete [y/n]?\")\n",
    "        if ans.lower() == 'y':\n",
    "            !rm -rf {exp_name}\n",
    "    else:\n",
    "        print(f\"Experiment doesn't exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-12 17:43:29.240219: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-06-12 17:43:29.240320: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/brazen/miniconda3/envs/ds-3.9/lib/python3.9/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n",
      "[nltk_data] Downloading package punkt to /home/brazen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/brazen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/brazen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/brazen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Logging into wanbd with key: aee284a72205e2d6787bd3ce266c5b9aefefa42c\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtony-pitchblack\u001b[0m (\u001b[33moverfit1010\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/brazen/.netrc\n",
      "Model will be saved as wandb artifact.\n",
      "Loading <utils.corus_dataset.CorusDataset object at 0x7f99b31cffa0> dataset...\n",
      "Restricted to 10 documents\n",
      "Collecting docs:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 56.48it/s]\n",
      "Done!\n",
      "Using predefined test split\n",
      "Using dynamically created val split\n",
      "using alternate train/validation split\n",
      "Computing embeddings:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:05<00:00,  1.38it/s]\n",
      "Computing embeddings:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.24it/s]\n",
      "Computing embeddings:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.27it/s]\n",
      "Train loader has: 7 documents\n",
      "Validation loader has: 1 documents\n",
      "Test loader has: 2 documents\n",
      "Seed set to 42\n",
      "Hyperparameters search active: True\n",
      "Running model with hyperparameters (set 0): (256, 2, 0.3, 0.7)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "Starting training...\n",
      "[rank: 0] Seed set to 42\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=gloo\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240612_174427-vno37zqe\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33menc_cointegrated/rubert-tiny2_opt_Adam_lr_0.001_bs_32_loss_CrossEntropy\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/overfit1010/lenta_BiLSTM_F1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/overfit1010/lenta_BiLSTM_F1/runs/vno37zqe\u001b[0m\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | BiLSTM | 2.7 M \n",
      "---------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "11.000    Total estimated model params size (MB)\n",
      "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]/home/brazen/miniconda3/envs/ds-3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "Sanity Checking DataLoader 0:   0%|                       | 0/1 [00:00<?, ?it/s]/home/brazen/miniconda3/envs/ds-3.9/lib/python3.9/site-packages/pytorch_lightning/profilers/pytorch.py:459: The PyTorch Profiler default schedule will be overridden as there is not enough steps to properly record traces.\n",
      "Sanity Checking DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.82it/s]STAGE:2024-06-12 17:44:32 952:952 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "/home/brazen/miniconda3/envs/ds-3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('valid_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/home/brazen/miniconda3/envs/ds-3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/home/brazen/miniconda3/envs/ds-3.9/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "Epoch 0:   0%|                                            | 0/1 [00:00<?, ?it/s]/home/brazen/miniconda3/envs/ds-3.9/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 7. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  0.27it/s, v_num=7zqe]\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.88it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  0.26it/s, v_num=7zqe]/home/brazen/miniconda3/envs/ds-3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('training_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  0.26it/s, v_num=7zqe, training_loss=0.694]Metric valid_loss improved. New best score: 0.579\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  0.22it/s, v_num=7zqe, training_loss=0.694]\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.78it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  0.21it/s, v_num=7zqe, training_loss=0.462]Metric valid_loss improved by 0.086 >= min_delta = 0.0. New best score: 0.493\n",
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  0.21it/s, v_num=7zqe, training_loss=0.462]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "STAGE:2024-06-12 17:44:41 952:952 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-06-12 17:44:41 952:952 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n",
      "ERROR:2024-06-12 17:44:58 952:952 CudaDeviceProperties.cpp:27] cudaGetDeviceCount failed with code 35\n",
      "[rank: 0] Seed set to 42\n",
      "Testing: |                                                | 0/? [00:00<?, ?it/s]/home/brazen/miniconda3/envs/ds-3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "Testing DataLoader 0:   0%|                               | 0/1 [00:00<?, ?it/s]/home/brazen/miniconda3/envs/ds-3.9/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "STAGE:2024-06-12 17:45:17 952:952 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.03it/s]/home/brazen/miniconda3/envs/ds-3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('Pk_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/home/brazen/miniconda3/envs/ds-3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('WD_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/home/brazen/miniconda3/envs/ds-3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('threshold', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/home/brazen/miniconda3/envs/ds-3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('test_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.95it/s]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "       Test metric             DataLoader 0\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "         Pk_loss            0.0786466896533966\n",
      "         WD_loss            0.0786466896533966\n",
      "        test_loss                   0.0\n",
      "        threshold                   0.5\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "STAGE:2024-06-12 17:45:17 952:952 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-06-12 17:45:17 952:952 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n",
      "Running model with hyperparameters (set 1): (256, 2, 0.3, 0.1)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "Starting training...\n",
      "[rank: 0] Seed set to 42\n",
      "/home/brazen/miniconda3/envs/ds-3.9/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /home/brazen/NSE-TopicSegmentation/lenta_dropout_search/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | BiLSTM | 2.7 M \n",
      "---------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "11.000    Total estimated model params size (MB)\n",
      "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]/home/brazen/miniconda3/envs/ds-3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "Sanity Checking DataLoader 0:   0%|                       | 0/1 [00:00<?, ?it/s]STAGE:2024-06-12 17:45:17 952:952 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "/home/brazen/miniconda3/envs/ds-3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/home/brazen/miniconda3/envs/ds-3.9/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  0.29it/s, v_num=7zqe]\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.76it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  0.26it/s, v_num=7zqe, training_loss=0.694]Metric valid_loss improved. New best score: 0.567\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  0.29it/s, v_num=7zqe, training_loss=0.694]\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.67it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  0.28it/s, v_num=7zqe, training_loss=0.436]Metric valid_loss improved by 0.085 >= min_delta = 0.0. New best score: 0.482\n",
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  0.28it/s, v_num=7zqe, training_loss=0.436]\n",
      "STAGE:2024-06-12 17:45:27 952:952 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-06-12 17:45:27 952:952 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n",
      "[rank: 0] Seed set to 42\n",
      "/home/brazen/miniconda3/envs/ds-3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "Testing DataLoader 0:   0%|                               | 0/1 [00:00<?, ?it/s]STAGE:2024-06-12 17:46:03 952:952 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.32it/s]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "       Test metric             DataLoader 0\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "         Pk_loss            0.0786466896533966\n",
      "         WD_loss            0.0786466896533966\n",
      "        test_loss                   0.0\n",
      "        threshold                   0.5\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "STAGE:2024-06-12 17:46:03 952:952 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-06-12 17:46:03 952:952 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             Pk_loss â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             WD_loss â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch â–â–â–…â–…â–ˆâ–â–â–…â–…â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           test_loss â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           threshold â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step â–â–â–…â–…â–ˆâ–â–â–…â–…â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       training_loss â–ˆâ–‚â–ˆâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          valid_loss â–ˆâ–‚â–‡â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             Pk_loss 0.07865\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             WD_loss 0.07865\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           test_loss 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           threshold 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       training_loss 0.4356\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          valid_loss 0.4819\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33menc_cointegrated/rubert-tiny2_opt_Adam_lr_0.001_bs_32_loss_CrossEntropy\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/overfit1010/lenta_BiLSTM_F1/runs/vno37zqe\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/overfit1010/lenta_BiLSTM_F1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240612_174427-vno37zqe/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "filepath = './train_fit.py'\n",
    "! chmod 755 {filepath}\n",
    "encoder_full_name = \"cointegrated/rubert-tiny2\"\n",
    "dataset_name = 'lenta'\n",
    "# exp_name = dataset_name + \n",
    "# exp_name = 'lenta_full'\n",
    "exp_name = 'lenta_dropout_search'\n",
    "delete_experiment(exp_name)\n",
    "\n",
    "! python {filepath} \\\n",
    "    --verbose \\\n",
    "    --dataset {dataset_name} --corus \\\n",
    "    -exp {exp_name} \\\n",
    "    --wandb --wandb_key aee284a72205e2d6787bd3ce266c5b9aefefa42c \\\n",
    "    --metric='F1' \\\n",
    "    -arc='BiLSTM' \\\n",
    "    --encoder={encoder_full_name} \\\n",
    "    --hidden_units=256 --num_layers=2 \\\n",
    "    --dropout_in 0.2 --dropout_out=0.1 \\\n",
    "    -lr=0.001 -bs=32 \\\n",
    "    --patience=5 \\\n",
    "    --threshold=0.5 \\\n",
    "    --max_epochs=2 \\\n",
    "    --max_docs_cnt=10 \\\n",
    "    --hyperparameters_search \\\n",
    "    -huss=256 -nlss=2 \\\n",
    "   -diss 0.3 -doss 0.7 0.1\\\n",
    "    # --save_embeddings \\\n",
    "#     --max_docs_frac=0.25 \\\n",
    "#     --max_epochs=100 \\\n",
    " ##     --embeddings_directory '../embeddings'\n",
    "    # --online_encoding\n",
    "#     --embeddings_directory='embeddings/'\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "key='aee284a72205e2d6787bd3ce266c5b9aefefa42c'\n",
    "wandb.login(key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "file_num = 0\n",
    "embeddings = [0] * 4\n",
    "path = '/home/brazen/NSE-TopicSegmentation/embeddings/lenta/cointegrated/rubert-tiny2_train/embeddings_0.npy'\n",
    "embeddings[int(file_num)] = np.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(229, 312)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "url = 'https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz'\n",
    "cmd = ['curl', '-O', url]\n",
    "out = subprocess.run(\n",
    "    cmd, text=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "if out.returncode != 0:\n",
    "    print(\"Error while loading file:\")\n",
    "    print(out.stdout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
